# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-1hlE_xWSFICqzwjjD9uSgpkXJBsFev6
"""

import yfinance as yf
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
import torch
import torch.nn as nn
import torch.optim as optim

# @title Default title text
# Define the ticker symbols of Moroccan companies
ticker_symbols = ["TATAMOTORS.NS","ASHOKLEY.NS","MARUTI.NS","HEROMOTOCO.NS","MRF.NS","BOSCHLTD.NS","BAJAJ-AUTO.NS","MOTHERSON.NS","EICHERMOT.NS","BALKRISIND.NS","BHARATFORG.NS","M&M.NS","TVSMOTOR.NS"]

# Specify the time period for data retrieval
start_date = '2018-01-01'
end_date = '2023-01-01'

# Create an empty DataFrame to store the data
historical_data = None

# Iterate over each ticker symbol and retrieve historical data
for ticker_symbol in ticker_symbols:
    # Download historical data for the current ticker symbol
    data = yf.download(ticker_symbol, start=start_date, end=end_date)

    # Extract adjusted close prices and rename the column
    adj_close = data['Adj Close'].rename(ticker_symbol)

    # Concatenate the adjusted close prices with existing data
    if historical_data is None:
        historical_data = adj_close
    else:
        historical_data = pd.concat([historical_data, adj_close], axis=1)

# Display the first few rows of the combined dataset
print(historical_data.head())

historical_data.info()

historical_data.head()

historical_data.plot(figsize=(12,6))
plt.title('Historical Adjusted Close Prices')
plt.show()

historical_data = historical_data.resample('D').mean()

historical_data.info()

print(historical_data.isnull().sum())

historical_data['TATAMOTORS.NS'] = historical_data['TATAMOTORS.NS'].ffill()
historical_data['ASHOKLEY.NS'] = historical_data['ASHOKLEY.NS'].ffill()
historical_data['MARUTI.NS'] = historical_data['MARUTI.NS'].ffill()
historical_data['HEROMOTOCO.NS'] = historical_data['HEROMOTOCO.NS'].ffill()
historical_data['MRF.NS'] = historical_data['MRF.NS'].ffill()
historical_data['BOSCHLTD.NS'] = historical_data['BOSCHLTD.NS'].ffill()
historical_data['MOTHERSON.NS'] = historical_data['MOTHERSON.NS'].ffill()
historical_data['EICHERMOT.NS'] = historical_data['EICHERMOT.NS'].ffill()
historical_data['BALKRISIND.NS'] = historical_data['BALKRISIND.NS'].ffill()
historical_data['BHARATFORG.NS'] = historical_data['BHARATFORG.NS'].ffill()
historical_data['M&M.NS'] = historical_data['M&M.NS'].ffill()
historical_data['TVSMOTOR.NS'] = historical_data['TVSMOTOR.NS'].ffill()

historical_data.plot(figsize=(12,6))
plt.title('Historical Adjusted Close Prices')
plt.show()

# Check for duplicates in the entire DataFrame
duplicates = historical_data.duplicated()

# Count the number of duplicates
num_duplicates = duplicates.sum()

# Display the number of duplicates
print("Number of duplicates:", num_duplicates)

# Show the rows with duplicates
duplicate_rows = historical_data[duplicates]
print("Duplicate rows:")
print(duplicate_rows)

# Create box plots for all columns in the DataFrame
historical_data.boxplot()

# Add title and labels
plt.title('Box Plot of All Columns')
plt.ylabel('Values')

# Rotate x-axis labels for better visibility (optional)
plt.xticks(rotation=45)

# Show the plot
plt.show()

# Exploring more ou cleaned dataframe
historical_data.describe()

correlation_matrix = historical_data.corr()
print(correlation_matrix)

# Correlation matrix of the returns
# Calculate monthly returns
monthly_returns = historical_data.pct_change()

# Drop the first row which contains NaN values
monthly_returns = monthly_returns.dropna()

# Create correlation matrix
correlation_matrix = monthly_returns.corr()

# Display correlation matrix
print(correlation_matrix)

# Heatmap of correlation matrix of the returns
plt.figure()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix')
plt.show()

# Pairplot for visualizing relationships between variables
sns.pairplot(historical_data)
plt.title('Pairplot of Variables')
plt.show()

# Define the scaler object
scaler = StandardScaler()

# Apply standardization to your data
standardized_data = scaler.fit_transform(historical_data)

# Convert the standardized array back to a DataFrame with original column names
historical_data = pd.DataFrame(standardized_data, columns=historical_data.columns)

device = torch.device('cuda')

historical_data.shape

def create_xy(data, time_step=1):
    X, y = [], []
    for i in range(len(data) - time_step - 1):
        end_ix = i + time_step
        if end_ix > len(data)-1:
            break
        seq_x = data[i:end_ix]
        X.append(seq_x)
        y.append(data[end_ix])
    return np.array(X), np.array(y)

# Evaluation metrics functions
def MAE(y_true, y_pred):
    return np.mean(np.abs(y_true - y_pred))

def MSE(y_true, y_pred):
    return np.mean((y_true - y_pred) ** 2)

def RMSE(y_true, y_pred):
    return np.sqrt(MSE(y_true, y_pred))

def MAPE(y_true, y_pred):
    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100

def MPE(y_true, y_pred):
    return np.mean((y_true - y_pred) / y_true) * 100

# Function to evaluate model performance
def evaluate_model(model, X, y):
    outputs = model(X)
    y_pred = outputs.detach().numpy()
    y_true = y.detach().numpy()

    mae = MAE(y_true, y_pred)
    mse = MSE(y_true, y_pred)
    rmse = RMSE(y_true, y_pred)
    mape = MAPE(y_true, y_pred)
    mpe = MPE(y_true, y_pred)

    return mae, mse, rmse, mape, mpe

assets=["TATAMOTORS.NS","ASHOKLEY.NS","MARUTI.NS","HEROMOTOCO.NS","MRF.NS","BOSCHLTD.NS","BAJAJ-AUTO.NS","MOTHERSON.NS","EICHERMOT.NS","BALKRISIND.NS","BHARATFORG.NS","M&M.NS","TVSMOTOR.NS"]
class LSTMModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):
        super(LSTMModel, self).__init__()
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers

        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        batch_size = x.size(0)
        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).requires_grad_()
        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).requires_grad_()
        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))
        out = self.fc(out[:, -1, :])
        out = out.squeeze()  # Squeeze out the extra dimension
        return out

# Define the loss function and optimizer
criterion = nn.MSELoss()

# Define the ratio for splitting data into train and validation sets
train_ratio = 0.8  # 80% of data for training, 20% for validation

# Training loop
epochs = 100
model_perf_df = pd.DataFrame(index=['MAE', 'MSE', 'RMSE', 'MAPE', 'MPE'], columns=assets)

# Initialize an empty dictionary to store predictions for each asset
all_predictions = {}

for ticker in historical_data.columns:
    # Split the data into features (X) and target (y)
    X, y = create_xy(historical_data[ticker].values, time_step=100)

    # Split data into training and validation sets
    split_index = int(len(X) * train_ratio)
    X_train, X_val = X[:split_index], X[split_index:]
    y_train, y_val = y[:split_index], y[split_index:]

    # Convert datasets to tensors
    X_train_tensors = torch.from_numpy(X_train).float()
    X_val_tensors = torch.from_numpy(X_val).float()
    y_train_tensors = torch.from_numpy(y_train).float()
    y_val_tensors = torch.from_numpy(y_val).float()

    # Add the missing input_dim dimension
    X_train_tensors = X_train_tensors.unsqueeze(-1)  # Add a new dimension at the end
    X_val_tensors = X_val_tensors.unsqueeze(-1)  # Add a new dimension at the end

    # Instantiate the model with input_dim=1
    model = LSTMModel(input_dim=1, hidden_dim=32, num_layers=2, output_dim=1)
    optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.001)  # Added weight_decay parameter to Adam optimizer

    for epoch in range(epochs):
        model.train()
        optimizer.zero_grad()
        outputs = model(X_train_tensors)
        loss = criterion(outputs, y_train_tensors)
        loss.backward()
        optimizer.step()
        if epoch % 10 == 0:
            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')

    # Evaluate the model on the validation set
    model.eval()
    with torch.no_grad():
        val_outputs = model(X_val_tensors)
        val_loss = criterion(val_outputs, y_val_tensors)
        print(f'Validation Loss: {val_loss.item():.4f}')

    # Store the predictions for the current asset
    all_predictions[ticker] = model(X_val_tensors).squeeze().detach().numpy()

    # Evaluate the model performance
    mae, mse, rmse, mape, mpe = evaluate_model(model, X_val_tensors, y_val_tensors)
    model_perf_df[ticker] = [mae, mse, rmse, mape, mpe]

model_perf_df.head()

# Plot true and predicted values for each asset
for ticker in historical_data.columns:
    # Get the true and predicted values for the current asset
    y_true = historical_data[ticker][-len(all_predictions[ticker]):]  # Get corresponding true values
    y_pred = all_predictions[ticker]

    # Plot true and predicted values
    plt.figure(figsize=(10, 6))
    plt.plot(y_true.index, y_true, label='True', marker='o')
    plt.plot(y_true.index, y_pred, label='Predicted', linestyle='--')

    # Add labels, legend, and title
    plt.xlabel('Date')
    plt.ylabel('Value')
    plt.title(f'True vs. Predicted Values for {ticker}')
    plt.legend()
    plt.grid(True)

    # Show plot
    plt.show()

